{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b4fea928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Microsoft Corporation.\n",
    "# Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4bc9ba",
   "metadata": {},
   "source": [
    "# Neo4j Import of GraphRAG Result Parquet files\n",
    "\n",
    "This notebook imports the results of the GraphRAG indexing process into the Neo4j Graph database for further processing, analysis or visualization. \n",
    "\n",
    "You can also build your own GenAI applications using Neo4j and a number of RAG strategies with LangChain, LlamaIndex, Haystack, and many other frameworks.\n",
    "See: https://neo4j.com/labs/genai-ecosystem\n",
    "\n",
    "Here is what the end result looks like:\n",
    "\n",
    "![](https://dev.assets.neo4j.com/wp-content/uploads/graphrag-neo4j-visualization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3924e246",
   "metadata": {},
   "source": [
    "## How does it work?\n",
    "\n",
    "The notebook loads the parquet files from the `output` folder of your indexing process and loads them into Pandas dataframes.\n",
    "It then uses a batching approach to send a slice of the data into Neo4j to create nodes and relationships and add relevant properties. The id-arrays on most entities are turned into relationships. \n",
    "\n",
    "All operations use MERGE, so they are idempotent, and you can run the script multiple times.\n",
    "\n",
    "If you need to clean out the database, you can run the following statement\n",
    "\n",
    "```cypher\n",
    "MATCH (n)\n",
    "CALL { WITH n DETACH DELETE n } IN TRANSACTIONS OF 25000 ROWS;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "adca1803",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPHRAG_FOLDER = \"/home/fei/git/graphrag/workdir/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "### Depedendencies\n",
    "\n",
    "We only need Pandas and the neo4j Python driver with the rust extension for faster network transport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b57beec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24948.57s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet pandas neo4j-rust-ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3eeee95f-e4f2-4052-94fb-a5dc8ab542ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from neo4j import GraphDatabase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307dd2f4",
   "metadata": {},
   "source": [
    "## Neo4j Installation\n",
    "\n",
    "You can create a free instance of Neo4j [online](https://console.neo4j.io). You get a credentials file that you can use for the connection credentials. You can also get an instance in any of the cloud marketplaces.\n",
    "\n",
    "If you want to install Neo4j locally either use [Neo4j Desktop](https://neo4j.com/download) or \n",
    "the official Docker image: `docker run -e NEO4J_AUTH=neo4j/password -p 7687:7687 -p 7474:7474 neo4j` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b6c15443-4acb-4f91-88ea-4e08abaa4c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection established.\n"
     ]
    }
   ],
   "source": [
    "NEO4J_URI = \"neo4j+s://e941d51d.databases.neo4j.io\"  # or neo4j+s://xxxx.databases.neo4j.io\n",
    "NEO4J_USERNAME = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"r2Fboc7Ul7R6MTiobAQva-rm0jYuvVtCjvbYJe_xS0A\"  # your password\n",
    "AUTH = (NEO4J_USERNAME, NEO4J_PASSWORD)\n",
    "NEO4J_DATABASE = \"neo4j\"\n",
    "\n",
    "# Create a Neo4j driver\n",
    "with GraphDatabase.driver(NEO4J_URI, auth=AUTH) as driver:\n",
    "    driver.verify_connectivity()\n",
    "    print(\"Connection established.\")\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a34adcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEO4J_URI = \"neo4j://localhost\"  # or neo4j+s://xxxx.databases.neo4j.io\n",
    "# NEO4J_USERNAME = \"neo4j\"\n",
    "# NEO4J_PASSWORD = \"password\"  # your password\n",
    "# AUTH = (NEO4J_USERNAME, NEO4J_PASSWORD)\n",
    "# NEO4J_DATABASE = \"neo4j\"\n",
    "\n",
    "# # Create a Neo4j driver\n",
    "# with GraphDatabase.driver(NEO4J_URI, auth=AUTH) as driver:\n",
    "#     driver.verify_connectivity()\n",
    "#     print(\"Connection established.\")\n",
    "\n",
    "# driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f37ab6",
   "metadata": {},
   "source": [
    "## Batched Import\n",
    "\n",
    "The batched import function takes a Cypher insert statement (needs to use the variable `value` for the row) and a dataframe to import.\n",
    "It will send by default 1k rows at a time as query parameter to the database to be inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d787bf7b-ac9b-4bfb-b140-a50a3fd205c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_import(statement, df, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Import a dataframe into Neo4j using a batched approach.\n",
    "\n",
    "    Parameters: statement is the Cypher query to execute, df is the dataframe to import, and batch_size is the number of rows to import in each batch.\n",
    "    \"\"\"\n",
    "    total = len(df)\n",
    "    start_s = time.time()\n",
    "    for start in range(0, total, batch_size):\n",
    "        batch = df.iloc[start : min(start + batch_size, total)]\n",
    "        result = driver.execute_query(\n",
    "            \"UNWIND $rows AS value \" + statement,\n",
    "            rows=batch.to_dict(\"records\"),\n",
    "            database_=NEO4J_DATABASE,\n",
    "        )\n",
    "        print(result.summary.counters)\n",
    "    print(f\"{total} rows in {time.time() - start_s} s.\")\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb45f42",
   "metadata": {},
   "source": [
    "## Indexes and Constraints\n",
    "\n",
    "Indexes in Neo4j are only used to find the starting points for graph queries, e.g. quickly finding two nodes to connect.\n",
    "Constraints exist to avoid duplicates, we create them mostly on id's of Entity types.\n",
    "\n",
    "We use some Types as markers with two underscores before and after to distinguish them from the actual entity types.\n",
    "\n",
    "The default relationship type here is `RELATED` but we could also infer a real relationship-type from the description or the types of the start and end-nodes.\n",
    "\n",
    "* `__Entity__`\n",
    "* `__Document__`\n",
    "* `__Chunk__`\n",
    "* `__Community__`\n",
    "* `__Covariate__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ed7f212e-9148-424c-adc6-d81db9f8e5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "create constraint chunk_id if not exists for (c:__Chunk__) require c.id is unique\n",
      "\n",
      "create constraint document_id if not exists for (d:__Document__) require d.id is unique\n",
      "\n",
      "create constraint entity_id if not exists for (c:__Community__) require c.community is unique\n",
      "\n",
      "create constraint entity_id if not exists for (e:__Entity__) require e.id is unique\n",
      "\n",
      "create constraint entity_title if not exists for (e:__Entity__) require e.name is unique\n",
      "\n",
      "create constraint entity_title if not exists for (e:__Covariate__) require e.title is unique\n",
      "\n",
      "create constraint related_id if not exists for ()-[rel:RELATED]->() require rel.id is unique\n"
     ]
    }
   ],
   "source": [
    "# create constraints, idempotent operation\n",
    "\n",
    "statements = [\n",
    "    \"\\ncreate constraint chunk_id if not exists for (c:__Chunk__) require c.id is unique\",\n",
    "    \"\\ncreate constraint document_id if not exists for (d:__Document__) require d.id is unique\",\n",
    "    \"\\ncreate constraint entity_id if not exists for (c:__Community__) require c.community is unique\",\n",
    "    \"\\ncreate constraint entity_id if not exists for (e:__Entity__) require e.id is unique\",\n",
    "    \"\\ncreate constraint entity_title if not exists for (e:__Entity__) require e.name is unique\",\n",
    "    \"\\ncreate constraint entity_title if not exists for (e:__Covariate__) require e.title is unique\",\n",
    "    \"\\ncreate constraint related_id if not exists for ()-[rel:RELATED]->() require rel.id is unique\",\n",
    "    \"\\n\",\n",
    "]\n",
    "\n",
    "for statement in statements:\n",
    "    if len((statement or \"\").strip()) > 0:\n",
    "        print(statement)\n",
    "        driver.execute_query(statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beea073b",
   "metadata": {},
   "source": [
    "## Import Process\n",
    "\n",
    "### Importing the Documents\n",
    "\n",
    "We're loading the parquet file for the documents and create nodes with their ids and add the title property.\n",
    "We don't need to store text_unit_ids as we can create the relationships and the text content is also contained in the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1ba023e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>647709a0d0e057670c5b0dbc015bb7ba03d6fedccb9a5f...</td>\n",
       "      <td>report.c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9f5a1185c3548f00b01f1686b1255c749c28fe24b4591f...</td>\n",
       "      <td>hashtable_utils.c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id              title\n",
       "0  647709a0d0e057670c5b0dbc015bb7ba03d6fedccb9a5f...           report.c\n",
       "1  9f5a1185c3548f00b01f1686b1255c749c28fe24b4591f...  hashtable_utils.c"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_df = pd.read_parquet(\n",
    "    f\"{GRAPHRAG_FOLDER}/create_final_documents.parquet\", columns=[\"id\", \"title\"]\n",
    ")\n",
    "doc_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "96391c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_contains_updates': True, 'labels_added': 38, 'nodes_created': 38, 'properties_set': 76}\n",
      "38 rows in 0.5366926193237305 s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import documents\n",
    "statement = \"\"\"\n",
    "MERGE (d:__Document__ {id:value.id})\n",
    "SET d += value {.title}\n",
    "\"\"\"\n",
    "\n",
    "batched_import(statement, doc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97bbadb",
   "metadata": {},
   "source": [
    "### Loading Text Units\n",
    "\n",
    "We load the text units, create a node per id and set the text and number of tokens.\n",
    "Then we connect them to the documents that we created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0d825626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>document_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dcaa484d1db860309b46a809929f5c1c27e7e53e0ab92b...</td>\n",
       "      <td>#ifndef _SSDBUFTABLE_H\\n#define _SSDBUFTABLE_H...</td>\n",
       "      <td>180</td>\n",
       "      <td>[0038e02189a4539ec6415c2b5d6be32a8947084c04302...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9f1e4d47e4e851180c13f165a57ae3143054795d165ed7...</td>\n",
       "      <td>#ifndef _STRAtEGY_H_\\n#define _STRAtEGY_H_\\n\\n...</td>\n",
       "      <td>134</td>\n",
       "      <td>[0f329dd40b18a57b4daba0923e508b26b3a7682cb90cb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id  \\\n",
       "0  dcaa484d1db860309b46a809929f5c1c27e7e53e0ab92b...   \n",
       "1  9f1e4d47e4e851180c13f165a57ae3143054795d165ed7...   \n",
       "\n",
       "                                                text  n_tokens  \\\n",
       "0  #ifndef _SSDBUFTABLE_H\\n#define _SSDBUFTABLE_H...       180   \n",
       "1  #ifndef _STRAtEGY_H_\\n#define _STRAtEGY_H_\\n\\n...       134   \n",
       "\n",
       "                                        document_ids  \n",
       "0  [0038e02189a4539ec6415c2b5d6be32a8947084c04302...  \n",
       "1  [0f329dd40b18a57b4daba0923e508b26b3a7682cb90cb...  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = pd.read_parquet(\n",
    "    f\"{GRAPHRAG_FOLDER}/create_final_text_units.parquet\",\n",
    "    columns=[\"id\", \"text\", \"n_tokens\", \"document_ids\"],\n",
    ")\n",
    "text_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ffd3d380-8710-46f5-b90a-04ed8482192c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_contains_updates': True, 'labels_added': 67, 'relationships_created': 67, 'nodes_created': 67, 'properties_set': 201}\n",
      "67 rows in 0.9977202415466309 s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement = \"\"\"\n",
    "MERGE (c:__Chunk__ {id:value.id})\n",
    "SET c += value {.text, .n_tokens}\n",
    "WITH c, value\n",
    "UNWIND value.document_ids AS document\n",
    "MATCH (d:__Document__ {id:document})\n",
    "MERGE (c)-[:PART_OF]->(d)\n",
    "\"\"\"\n",
    "\n",
    "batched_import(statement, text_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01b2094",
   "metadata": {},
   "source": [
    "### Loading Nodes\n",
    "\n",
    "For the nodes we store id, name, description, embedding (if available), human readable id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2392f9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>human_readable_id</th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>description</th>\n",
       "      <th>text_unit_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87eaa567-cc2e-4f6b-a3a5-3d1ca18eb87c</td>\n",
       "      <td>0</td>\n",
       "      <td>SSDBUFHASHBUCKET</td>\n",
       "      <td>DATA STRUCTURE</td>\n",
       "      <td>A data structure used to store information in ...</td>\n",
       "      <td>[dcaa484d1db860309b46a809929f5c1c27e7e53e0ab92...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b170cde1-355e-469e-bb6e-baa9680eba21</td>\n",
       "      <td>1</td>\n",
       "      <td>SSD_BUF_HASHTABLE</td>\n",
       "      <td>VARIABLE</td>\n",
       "      <td>A variable that points to the first SSDBufHash...</td>\n",
       "      <td>[dcaa484d1db860309b46a809929f5c1c27e7e53e0ab92...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  human_readable_id              title  \\\n",
       "0  87eaa567-cc2e-4f6b-a3a5-3d1ca18eb87c                  0   SSDBUFHASHBUCKET   \n",
       "1  b170cde1-355e-469e-bb6e-baa9680eba21                  1  SSD_BUF_HASHTABLE   \n",
       "\n",
       "             type                                        description  \\\n",
       "0  DATA STRUCTURE  A data structure used to store information in ...   \n",
       "1        VARIABLE  A variable that points to the first SSDBufHash...   \n",
       "\n",
       "                                       text_unit_ids  \n",
       "0  [dcaa484d1db860309b46a809929f5c1c27e7e53e0ab92...  \n",
       "1  [dcaa484d1db860309b46a809929f5c1c27e7e53e0ab92...  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_df = pd.read_parquet(\n",
    "    f\"{GRAPHRAG_FOLDER}/create_final_entities.parquet\",\n",
    "    columns=[\n",
    "        \"id\",\n",
    "        \"human_readable_id\",\n",
    "        \"title\",\n",
    "        \"type\",\n",
    "        \"description\",\n",
    "        \"text_unit_ids\",\n",
    "    ],\n",
    ")\n",
    "entity_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1d038114-0714-48ee-a48a-c421cd539661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity_statement = \"\"\"\n",
    "# MERGE (e:__Entity__ {id:value.id})\n",
    "# SET e += value {.human_readable_id, .description, title:replace(value.title,'\"','')}\n",
    "# WITH e, value\n",
    "# CALL db.create.setNodeVectorProperty(e, \"description\", value.description)\n",
    "# CALL apoc.create.addLabels(e, case when coalesce(value.type,\"\") = \"\" then [] else [apoc.text.upperCamelCase(replace(value.type,'\"',''))] end) yield node\n",
    "# UNWIND value.text_unit_ids AS text_unit\n",
    "# MATCH (c:__Chunk__ {id:text_unit})\n",
    "# MERGE (c)-[:HAS_ENTITY]->(e)\n",
    "# \"\"\"\n",
    "\n",
    "# batched_import(entity_statement, entity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8e5fe82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_contains_updates': True, 'labels_added': 924, 'relationships_created': 1008, 'nodes_created': 474, 'properties_set': 1896}\n",
      "474 rows in 1.448024034500122 s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "474"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_statement = \"\"\"\n",
    "MERGE (e:__Entity__ {id:value.id})\n",
    "SET e += value {.human_readable_id, .description, title:replace(value.title,'\"','')}\n",
    "WITH e, value\n",
    "FOREACH (label IN CASE WHEN value.type IS NOT NULL AND value.type <> '' \n",
    "         THEN [value.type] ELSE [] END | \n",
    "         SET e:`${label}`)\n",
    "WITH e, value\n",
    "UNWIND value.text_unit_ids AS text_unit\n",
    "MATCH (c:__Chunk__ {id:text_unit})\n",
    "MERGE (c)-[:HAS_ENTITY]->(e)\n",
    "\"\"\"\n",
    "\n",
    "batched_import(entity_statement, entity_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018d4f87",
   "metadata": {},
   "source": [
    "### Import Relationships\n",
    "\n",
    "For the relationships we find the source and target node by name, using the base `__Entity__` type.\n",
    "After creating the `RELATED` relationships, we set the description as attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b347a047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>combined_degree</th>\n",
       "      <th>weight</th>\n",
       "      <th>human_readable_id</th>\n",
       "      <th>description</th>\n",
       "      <th>text_unit_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SSDBUFHASHBUCKET</td>\n",
       "      <td>SSD_BUF_HASHTABLE</td>\n",
       "      <td>af0e92ee-f14c-42aa-af87-eb32d4cdfc90</td>\n",
       "      <td>11</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>ssd_buf_hashtable is a pointer to the first SS...</td>\n",
       "      <td>[dcaa484d1db860309b46a809929f5c1c27e7e53e0ab92...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SSDBUFHASHBUCKET</td>\n",
       "      <td>HASHTAB_GETHASHCODE</td>\n",
       "      <td>2509020c-bfe8-4a18-ab83-a01433fab524</td>\n",
       "      <td>10</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>HashTab_GetHashCode function uses SSDBufHashBu...</td>\n",
       "      <td>[dcaa484d1db860309b46a809929f5c1c27e7e53e0ab92...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             source               target  \\\n",
       "0  SSDBUFHASHBUCKET    SSD_BUF_HASHTABLE   \n",
       "1  SSDBUFHASHBUCKET  HASHTAB_GETHASHCODE   \n",
       "\n",
       "                                     id  combined_degree  weight  \\\n",
       "0  af0e92ee-f14c-42aa-af87-eb32d4cdfc90               11    20.0   \n",
       "1  2509020c-bfe8-4a18-ab83-a01433fab524               10     7.0   \n",
       "\n",
       "   human_readable_id                                        description  \\\n",
       "0                  0  ssd_buf_hashtable is a pointer to the first SS...   \n",
       "1                  1  HashTab_GetHashCode function uses SSDBufHashBu...   \n",
       "\n",
       "                                       text_unit_ids  \n",
       "0  [dcaa484d1db860309b46a809929f5c1c27e7e53e0ab92...  \n",
       "1  [dcaa484d1db860309b46a809929f5c1c27e7e53e0ab92...  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_df = pd.read_parquet(\n",
    "    f\"{GRAPHRAG_FOLDER}/create_final_relationships.parquet\",\n",
    "    columns=[\n",
    "        \"source\",\n",
    "        \"target\",\n",
    "        \"id\",\n",
    "        \"combined_degree\",\n",
    "        \"weight\",\n",
    "        \"human_readable_id\",\n",
    "        \"description\",\n",
    "        \"text_unit_ids\",\n",
    "    ],\n",
    ")\n",
    "rel_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "27900c01-89e1-4dec-9d5c-c07317c68baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_contains_updates': True, 'relationships_created': 653, 'properties_set': 3265}\n",
      "653 rows in 2.0415780544281006 s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "653"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_statement = \"\"\"\n",
    "    MATCH (source:__Entity__ {title:replace(value.source,'\"','')})\n",
    "    MATCH (target:__Entity__ {title:replace(value.target,'\"','')})\n",
    "    // not necessary to merge on id as there is only one relationship per pair\n",
    "    MERGE (source)-[rel:RELATED {id: value.id}]->(target)\n",
    "    SET rel += value {.rank, .weight, .human_readable_id, .description, .text_unit_ids}\n",
    "    RETURN count(*) as createdRels\n",
    "\"\"\"\n",
    "\n",
    "batched_import(rel_statement, rel_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6365dd7",
   "metadata": {},
   "source": [
    "### Importing Communities\n",
    "\n",
    "For communities we import their id, title, level.\n",
    "We connect the `__Community__` nodes to the start and end nodes of the relationships they refer to.\n",
    "\n",
    "Connecting them to the chunks they orignate from is optional, as the entites are already connected to the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c2fab66c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>level</th>\n",
       "      <th>title</th>\n",
       "      <th>text_unit_ids</th>\n",
       "      <th>relationship_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73cbeacb-7f76-46d1-a025-e9e70d7c7396</td>\n",
       "      <td>0</td>\n",
       "      <td>Community 0</td>\n",
       "      <td>[0806dbf3d6f7481790423036bc2afd6b54647c4282307...</td>\n",
       "      <td>[0309cdc2-9383-4b4e-be78-0675a75004d2, 0a9b46a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>843a8571-eebf-4504-9c21-d84a5aefd271</td>\n",
       "      <td>0</td>\n",
       "      <td>Community 1</td>\n",
       "      <td>[25dc39b6f5cca06b4b96f4492bfbe343263f5f2770505...</td>\n",
       "      <td>[38489977-ffba-4057-88fd-cf84c78e3be4, 3a0a40c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  level        title  \\\n",
       "0  73cbeacb-7f76-46d1-a025-e9e70d7c7396      0  Community 0   \n",
       "1  843a8571-eebf-4504-9c21-d84a5aefd271      0  Community 1   \n",
       "\n",
       "                                       text_unit_ids  \\\n",
       "0  [0806dbf3d6f7481790423036bc2afd6b54647c4282307...   \n",
       "1  [25dc39b6f5cca06b4b96f4492bfbe343263f5f2770505...   \n",
       "\n",
       "                                    relationship_ids  \n",
       "0  [0309cdc2-9383-4b4e-be78-0675a75004d2, 0a9b46a...  \n",
       "1  [38489977-ffba-4057-88fd-cf84c78e3be4, 3a0a40c...  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "community_df = pd.read_parquet(\n",
    "    f\"{GRAPHRAG_FOLDER}/create_final_communities.parquet\",\n",
    "    columns=[\"id\", \"level\", \"title\", \"text_unit_ids\", \"relationship_ids\"],\n",
    ")\n",
    "\n",
    "community_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1351f7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_contains_updates': True, 'labels_added': 85, 'relationships_created': 766, 'nodes_created': 85, 'properties_set': 255}\n",
      "85 rows in 2.3909716606140137 s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement = \"\"\"\n",
    "MERGE (c:__Community__ {community:value.id})\n",
    "SET c += value {.level, .title}\n",
    "/*\n",
    "UNWIND value.text_unit_ids as text_unit_id\n",
    "MATCH (t:__Chunk__ {id:text_unit_id})\n",
    "MERGE (c)-[:HAS_CHUNK]->(t)\n",
    "WITH distinct c, value\n",
    "*/\n",
    "WITH *\n",
    "UNWIND value.relationship_ids as rel_id\n",
    "MATCH (start:__Entity__)-[:RELATED {id:rel_id}]->(end:__Entity__)\n",
    "MERGE (start)-[:IN_COMMUNITY]->(c)\n",
    "MERGE (end)-[:IN_COMMUNITY]->(c)\n",
    "RETURn count(distinct c) as createdCommunities\n",
    "\"\"\"\n",
    "\n",
    "batched_import(statement, community_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9adf50",
   "metadata": {},
   "source": [
    "### Importing Community Reports\n",
    "\n",
    "Fo the community reports we create nodes for each communitiy set the id, community, level, title, summary, rank, and rank_explanation and connect them to the entities they are about.\n",
    "For the findings we create the findings in context of the communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1be9e7a9-69ee-406b-bce5-95a9c41ecffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>community</th>\n",
       "      <th>level</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>findings</th>\n",
       "      <th>rank</th>\n",
       "      <th>rank_explanation</th>\n",
       "      <th>full_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ce0cedd9eb654e48b68cf66540d34ad0</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>Optimizing SSD Cache Management in Software En...</td>\n",
       "      <td>This report delves into the intricate network ...</td>\n",
       "      <td>[{'explanation': 'The HIT_SAC function is pivo...</td>\n",
       "      <td>9.5</td>\n",
       "      <td>The technical depth and direct applicability o...</td>\n",
       "      <td># Optimizing SSD Cache Management in Software ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b4691d06dd5e46d996aa06d364bb3656</td>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>DSCPTR_SAC: A Keystone in Software Engineering...</td>\n",
       "      <td>The DSCPTR_SAC data structure is pivotal in th...</td>\n",
       "      <td>[{'explanation': 'The DSCPTR_SAC data structur...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>The detailed structure and relationships of DS...</td>\n",
       "      <td># DSCPTR_SAC: A Keystone in Software Engineeri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  community  level  \\\n",
       "0  ce0cedd9eb654e48b68cf66540d34ad0         64      2   \n",
       "1  b4691d06dd5e46d996aa06d364bb3656         65      2   \n",
       "\n",
       "                                               title  \\\n",
       "0  Optimizing SSD Cache Management in Software En...   \n",
       "1  DSCPTR_SAC: A Keystone in Software Engineering...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  This report delves into the intricate network ...   \n",
       "1  The DSCPTR_SAC data structure is pivotal in th...   \n",
       "\n",
       "                                            findings  rank  \\\n",
       "0  [{'explanation': 'The HIT_SAC function is pivo...   9.5   \n",
       "1  [{'explanation': 'The DSCPTR_SAC data structur...   9.0   \n",
       "\n",
       "                                    rank_explanation  \\\n",
       "0  The technical depth and direct applicability o...   \n",
       "1  The detailed structure and relationships of DS...   \n",
       "\n",
       "                                        full_content  \n",
       "0  # Optimizing SSD Cache Management in Software ...  \n",
       "1  # DSCPTR_SAC: A Keystone in Software Engineeri...  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "community_report_df = pd.read_parquet(\n",
    "    f\"{GRAPHRAG_FOLDER}/create_final_community_reports.parquet\",\n",
    "    columns=[\n",
    "        \"id\",\n",
    "        \"community\",\n",
    "        \"level\",\n",
    "        \"title\",\n",
    "        \"summary\",\n",
    "        \"findings\",\n",
    "        \"rank\",\n",
    "        \"rank_explanation\",\n",
    "        \"full_content\",\n",
    "    ],\n",
    ")\n",
    "community_report_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5c6ed591-f98c-4403-9fde-8d4cb4c01cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_contains_updates': True, 'labels_added': 520, 'relationships_created': 435, 'nodes_created': 520, 'properties_set': 1900}\n",
      "85 rows in 0.594315767288208 s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import communities\n",
    "community_statement = \"\"\"\n",
    "MERGE (c:__Community__ {community:value.community})\n",
    "SET c += value {.level, .title, .rank, .rank_explanation, .full_content, .summary}\n",
    "WITH c, value\n",
    "UNWIND range(0, size(value.findings)-1) AS finding_idx\n",
    "WITH c, value, finding_idx, value.findings[finding_idx] as finding\n",
    "MERGE (c)-[:HAS_FINDING]->(f:Finding {id:finding_idx})\n",
    "SET f += finding\n",
    "\"\"\"\n",
    "batched_import(community_statement, community_report_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a1a24a",
   "metadata": {},
   "source": [
    "### Importing Covariates\n",
    "\n",
    "Covariates are for instance claims on entities, we connect them to the chunks where they originate from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "523bed92-d12c-4fc4-aa44-6c62321b36bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/fei/git/graphrag/workdir/output/create_final_covariates.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cov_df \u001b[38;5;241m=\u001b[39m (\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mGRAPHRAG_FOLDER\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/create_final_covariates.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#                         columns=[\"id\",\"text_unit_id\"])\u001b[39;00m\n\u001b[1;32m      3\u001b[0m cov_df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/git/graphrag/.venv/lib/python3.12/site-packages/pandas/io/parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/graphrag/.venv/lib/python3.12/site-packages/pandas/io/parquet.py:267\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    265\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    275\u001b[0m         path_or_handle,\n\u001b[1;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    280\u001b[0m     )\n",
      "File \u001b[0;32m~/git/graphrag/.venv/lib/python3.12/site-packages/pandas/io/parquet.py:140\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    130\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/git/graphrag/.venv/lib/python3.12/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/fei/git/graphrag/workdir/output/create_final_covariates.parquet'"
     ]
    }
   ],
   "source": [
    "cov_df = (pd.read_parquet(f\"{GRAPHRAG_FOLDER}/create_final_covariates.parquet\"),)\n",
    "#                         columns=[\"id\",\"text_unit_id\"])\n",
    "cov_df.head(2)\n",
    "# Subject id do not match entity ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e064234-5fce-448e-8bb4-ab2f35699049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_contains_updates': True, 'labels_added': 89, 'relationships_created': 89, 'nodes_created': 89, 'properties_set': 1061}\n",
      "89 rows in 0.13370895385742188 s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import covariates\n",
    "cov_statement = \"\"\"\n",
    "MERGE (c:__Covariate__ {id:value.id})\n",
    "SET c += apoc.map.clean(value, [\"text_unit_id\", \"document_ids\", \"n_tokens\"], [NULL, \"\"])\n",
    "WITH c, value\n",
    "MATCH (ch:__Chunk__ {id: value.text_unit_id})\n",
    "MERGE (ch)-[:HAS_COVARIATE]->(c)\n",
    "\"\"\"\n",
    "batched_import(cov_statement, cov_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00340bae",
   "metadata": {},
   "source": [
    "### Visualize your data\n",
    "\n",
    "You can now [Open] Neo4j on Aura, you need to log in with either SSO or your credentials.\n",
    "\n",
    "Or open https://workspace-preview.neo4j.io and connect to your local instance, remember the URI is `neo4j://localhost` and `neo4j` as username and `password` as password.\n",
    "\n",
    "In \"Explore\" you can explore by using visual graph patterns and then explore and expand further.\n",
    "\n",
    "In \"Query\", you can open the left sidebar and explore by clicking on the nodes and relationships.\n",
    "You can also use the co-pilot to generate Cypher queries for your, here are some examples.\n",
    "\n",
    "#### Show a few `__Entity__` nodes and their relationships (Entity Graph)\n",
    "\n",
    "```cypher\n",
    "MATCH path = (:__Entity__)-[:RELATED]->(:__Entity__)\n",
    "RETURN path LIMIT 200\n",
    "```\n",
    "\n",
    "#### Show the Chunks and the Document (Lexical Graph)\n",
    "\n",
    "```cypher\n",
    "MATCH (d:__Document__) WITH d LIMIT 1\n",
    "MATCH path = (d)<-[:PART_OF]-(c:__Chunk__)\n",
    "RETURN path LIMIT 100\n",
    "```\n",
    "\n",
    "####  Show a Community and it's Entities\n",
    "\n",
    "```cypher\n",
    "MATCH (c:__Community__) WITH c LIMIT 1\n",
    "MATCH path = (c)<-[:IN_COMMUNITY]-()-[:RELATED]-(:__Entity__)\n",
    "RETURN path LIMIT 100\n",
    "```\n",
    "\n",
    "#### Show everything\n",
    "\n",
    "```cypher\n",
    "MATCH (d:__Document__) WITH d LIMIT 1\n",
    "MATCH path = (d)<-[:PART_OF]-(:__Chunk__)-[:HAS_ENTIY]->()-[:RELATED]-()-[:IN_COMMUNITY]->()\n",
    "RETURN path LIMIT 250\n",
    "```\n",
    "\n",
    "We showed the visualization of this last query at the beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aa8529",
   "metadata": {},
   "source": [
    "If you have questions, feel free to reach out in the GraphRAG discord server: \n",
    "https://discord.gg/graphrag"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
